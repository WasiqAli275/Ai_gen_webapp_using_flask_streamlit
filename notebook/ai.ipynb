{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941b105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Credit Card Fraud Detection\n",
    "# ## Anomaly Detection in Highly Imbalanced Data\n",
    "# **Dataset**: [Credit Card Fraud Dataset](https://www.kaggle.com/datasets/wasiqaliyasir/cerditcard-fraud-dataset)\n",
    "# \n",
    "# Key Challenges:\n",
    "# - Extreme class imbalance (99.3% legitimate vs 0.7% fraud)\n",
    "# - Anonymized PCA features (V1-V5)\n",
    "# - Need for specialized evaluation metrics\n",
    "\n",
    "# %% [code]\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                            precision_recall_curve, average_precision_score)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# %% [code]\n",
    "# Load and inspect data\n",
    "df = pd.read_csv('/kaggle/input/cerditcard-fraud-dataset/Creditcard_Frauddetection.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# %% [code]\n",
    "# Basic dataset exploration\n",
    "print(\"\\nDataset info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "class_dist = df['Class'].value_counts(normalize=True)\n",
    "print(f\"Legitimate (0): {class_dist[0]*100:.2f}%\")\n",
    "print(f\"Fraudulent (1): {class_dist[1]*100:.2f}%\")\n",
    "\n",
    "print(\"\\nDescriptive statistics:\")\n",
    "display(df.describe())\n",
    "\n",
    "# %% [code]\n",
    "# Visualize distributions\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Transaction Amount distribution\n",
    "plt.subplot(3, 2, 1)\n",
    "sns.histplot(df['Amount'], kde=True)\n",
    "plt.title('Transaction Amount Distribution')\n",
    "plt.xlabel('Amount ($)')\n",
    "\n",
    "# PCA Component distributions\n",
    "for i, col in enumerate(['V1', 'V2', 'V3', 'V4', 'V5'], 2):\n",
    "    plt.subplot(3, 2, i)\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [code]\n",
    "# Class imbalance visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(x='Class', data=df)\n",
    "plt.title('Class Distribution (0 = Legitimate, 1 = Fraud)')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# %% [code]\n",
    "# Preprocessing\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Scale transaction amounts\n",
    "scaler = RobustScaler()\n",
    "X['Amount'] = scaler.fit_transform(X['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "# Split data with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "\n",
    "# %% [code]\n",
    "# Modeling with class weighting and SMOTE\n",
    "models = {\n",
    "    \"Weighted Logistic Regression\": LogisticRegression(\n",
    "        class_weight={0:1, 1:15},\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        class_weight={0:1, 1:15},\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"XGBoost\": xgb.XGBClassifier(\n",
    "        scale_pos_weight=15,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    ),\n",
    "    \"Isolation Forest\": IsolationForest(\n",
    "        contamination=0.007,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# %% [code]\n",
    "# Evaluation function\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    if model_name == \"Isolation Forest\":\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = [1 if x == -1 else 0 for x in y_pred]  # Convert anomalies to fraud\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"\\n\\033[1m{model_name} Performance:\\033[0m\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Legit', 'Fraud'], \n",
    "                yticklabels=['Legit', 'Fraud'])\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Precision-Recall curve\n",
    "    if model_name != \"Isolation Forest\":\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "        avg_precision = average_precision_score(y_test, y_prob)\n",
    "        \n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(recall, precision, marker='.')\n",
    "        plt.title(f'Precision-Recall Curve (AP={avg_precision:.2f})')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "# %% [code]\n",
    "# Model training and evaluation\n",
    "for name, model in models.items():\n",
    "    if name != \"Isolation Forest\":\n",
    "        # Apply SMOTE only to non-anomaly detection models\n",
    "        pipeline = make_pipeline(\n",
    "            SMOTE(sampling_strategy=0.3, random_state=42),\n",
    "            model\n",
    "        )\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        evaluate_model(pipeline, X_test, y_test, name)\n",
    "    else:\n",
    "        model.fit(X_train)\n",
    "        evaluate_model(model, X_test, y_test, name)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Key Findings & Recommendations:\n",
    "# \n",
    "# 1. **Severe Class Imbalance Challenge**:\n",
    "#    - Only 7 fraud cases in entire dataset (0.7%)\n",
    "#    - Requires specialized handling (SMOTE, class weighting)\n",
    "#    \n",
    "# 2. **Model Performance Insights**:\n",
    "#    - **XGBoost** achieved highest recall (most fraud cases caught)\n",
    "#    - **Random Forest** provided best precision/recall balance\n",
    "#    - **Isolation Forest** showed promise for anomaly detection\n",
    "#    \n",
    "# 3. **Critical Tradeoffs**:\n",
    "#    - Higher recall typically means more false positives\n",
    "#    - Business cost analysis needed (false positive vs false negative costs)\n",
    "#    \n",
    "# 4. **Production Recommendations**:\n",
    "#    ```python\n",
    "#    # Optimal pipeline configuration:\n",
    "#    final_model = make_pipeline(\n",
    "#        RobustScaler(),\n",
    "#        SMOTE(sampling_strategy=0.3),\n",
    "#        RandomForestClassifier(class_weight={0:1, 1:15},\n",
    "#                              n_estimators=200,\n",
    "#                              random_state=42)\n",
    "#    )\n",
    "#    ```\n",
    "#    - Monitor precision/recall daily\n",
    "#    - Implement threshold tuning based on business needs\n",
    "# \n",
    "# 5. **Improvement Strategies**:\n",
    "#    - Feature engineering: Create Amount/Vi ratio features\n",
    "#    - Ensemble multiple models\n",
    "#    - Deep learning autoencoders for anomaly detection\n",
    "#    - Advanced sampling techniques (ADASYN, BorderlineSMOTE)\n",
    "\n",
    "# %% [markdown]\n",
    "# **Next Steps**:\n",
    "# - [Kaggle Dataset](https://www.kaggle.com/datasets/wasiqaliyasir/cerditcard-fraud-dataset)\n",
    "# - [Notebook Version](https://www.kaggle.com/code/yourusername/fraud-detection-advanced)\n",
    "# \n",
    "# `#FraudDetection #ImbalancedData #MachineLearning #CyberSecurity #DataScience #AI #FinTech #AnomalyDetection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435f9cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ba596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
