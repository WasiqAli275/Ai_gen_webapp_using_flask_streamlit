{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Student Academic Trends \u2014 Full Analysis\n", "\n", "This notebook performs a complete analysis of the Kaggle dataset **Analyzing Student Academic Trends**. It is designed to run on Kaggle (or locally after downloading the CSV). Steps included:\n", "1. Import libraries\n", "2. Load dataset\n", "3. Data inspection & cleaning\n", "4. Exploratory Data Analysis (9 combined plots using Matplotlib & Seaborn)\n", "5. Train multiple ML models (Random Forest, Ridge, Lasso, Decision Tree, KNN, Linear Regression, Gradient Boosting, SVR)\n", "6. Model evaluation and comparison (RMSE, MAE, R2) with 4 combined plots (barplot, lineplot, boxplot, ROC-like curves)\n", "\n", "> **Note:** The notebook tries to load the dataset from Kaggle's input folder. If you run this locally, replace the `DATA_PATH` variable with the CSV file path."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n", "from sklearn.tree import DecisionTreeRegressor\n", "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n", "from sklearn.neighbors import KNeighborsRegressor\n", "from sklearn.svm import SVR\n", "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, roc_curve, auc\n", "from sklearn.preprocessing import StandardScaler\n", "sns.set(style='whitegrid')\n", "plt.rcParams['figure.figsize'] = (10,6)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["DATA_PATH = '/kaggle/input/analyzing-student-academic-trends/analyzing_student_academic_trends.csv'\n", "try:\n", "    df = pd.read_csv(DATA_PATH)\n", "    print('Loaded dataset from Kaggle input')\n", "except Exception as e:\n", "    print('Could not load from Kaggle path \u2014 falling back to attempting local file or creating synthetic sample. Error:', e)\n", "    try:\n", "        df = pd.read_csv('analyzing_student_academic_trends.csv')\n", "        print('Loaded dataset from local working directory')\n", "    except Exception:\n", "        print('Local file not found \u2014 creating a synthetic sample (200 rows) for demonstration')\n", "        np.random.seed(42)\n", "        n = 200\n", "        df = pd.DataFrame({\n", "            'hours_studied': np.random.uniform(0, 10, n),\n", "            'sleep_hours': np.random.uniform(4, 10, n),\n", "            'attendance_percent': np.random.uniform(50, 100, n),\n", "            'previous_scores': np.random.uniform(40, 100, n)\n", "        })\n", "        df['exam_score'] = (5*df['hours_studied'] + 0.3*df['attendance_percent'] + 0.4*df['previous_scores']\n", "                            + 1.5*(df['sleep_hours']-7) + np.random.normal(0,5,n)).clip(0,100)\n", "        df.reset_index(drop=True, inplace=True)\n", "print('\\nDataset shape:', df.shape)\n", "display(df.head())\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Quick Data Inspection & Cleaning\n", "- Check dtypes, missing values, basic statistics\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(df.dtypes)\n", "print('\\nMissing values:')\n", "print(df.isnull().sum())\n", "display(df.describe())\n", "required = ['hours_studied','sleep_hours','attendance_percent','previous_scores','exam_score']\n", "for col in required:\n", "    if col not in df.columns:\n", "        raise ValueError(f\"Required column '{col}' not found in dataset.\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exploratory Data Analysis (9 combined plots)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df['study_cat'] = pd.cut(df['hours_studied'], bins=[-0.01,2.5,5,7.5,10], labels=['Very Low','Low','Medium','High'])\n", "df['sleep_cat'] = pd.cut(df['sleep_hours'], bins=[3.9,6,7.5,10], labels=['Short','Normal','Long'])\n", "fig, axes = plt.subplots(3,3, figsize=(18,14))\n", "sns.scatterplot(x='hours_studied', y='exam_score', data=df, ax=axes[0,0])\n", "sns.regplot(x='hours_studied', y='exam_score', data=df, scatter=False, ax=axes[0,0], color='red')\n", "axes[0,0].set_title('Hours Studied vs Exam Score (scatter + reg)')\n", "\n", "sns.scatterplot(x='attendance_percent', y='exam_score', data=df, ax=axes[0,1])\n", "sns.regplot(x='attendance_percent', y='exam_score', data=df, scatter=False, ax=axes[0,1], color='red')\n", "axes[0,1].set_title('Attendance vs Exam Score (scatter + reg)')\n", "\n", "sns.scatterplot(x='previous_scores', y='exam_score', data=df, ax=axes[0,2])\n", "sns.regplot(x='previous_scores', y='exam_score', data=df, scatter=False, ax=axes[0,2], color='red')\n", "axes[0,2].set_title('Previous Scores vs Exam Score (scatter + reg)')\n", "\n", "sns.histplot(df['hours_studied'], kde=True, ax=axes[1,0])\n", "ax2 = axes[1,0].twinx()\n", "sns.boxplot(x='hours_studied', data=df, ax=ax2, width=0.15)\n", "axes[1,0].set_title('Hours Studied: Hist + Box')\n", "ax2.set_yticks([])\n", "\n", "sns.histplot(df['exam_score'], kde=True, ax=axes[1,1])\n", "ax2 = axes[1,1].twinx()\n", "sns.boxplot(x='exam_score', data=df, ax=ax2, width=0.15)\n", "axes[1,1].set_title('Exam Score: Hist + Box')\n", "ax2.set_yticks([])\n", "\n", "order = df.groupby('study_cat')['exam_score'].mean().sort_values().index\n", "sns.barplot(x='study_cat', y='exam_score', data=df, order=order, ax=axes[1,2])\n", "sns.stripplot(x='study_cat', y='exam_score', data=df, order=order, ax=axes[1,2], color='black', alpha=0.5)\n", "axes[1,2].set_title('Avg Exam by Study Category (bar + strip)')\n", "\n", "avg_by_hours = df.groupby(df['hours_studied'].round())['exam_score'].mean()\n", "sns.lineplot(x=avg_by_hours.index, y=avg_by_hours.values, marker='o', ax=axes[2,0])\n", "sns.scatterplot(x='hours_studied', y='exam_score', data=df, alpha=0.3, ax=axes[2,0])\n", "axes[2,0].set_title('Avg Exam by Rounded Hours (line + scatter)')\n", "\n", "sns.violinplot(x='sleep_cat', y='exam_score', data=df, ax=axes[2,1])\n", "sns.swarmplot(x='sleep_cat', y='exam_score', data=df, ax=axes[2,1], color='k', alpha=0.6)\n", "axes[2,1].set_title('Exam Score by Sleep Category (violin + swarm)')\n", "\n", "corr = df[['hours_studied','sleep_hours','attendance_percent','previous_scores','exam_score']].corr()\n", "sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', ax=axes[2,2])\n", "axes[2,2].set_title('Correlation Matrix')\n", "\n", "plt.tight_layout()\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Machine Learning Models\n", "Train multiple models on selected features and evaluate performance."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = df[['hours_studied','sleep_hours','attendance_percent','previous_scores']]\n", "y = df['exam_score']\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "scaler = StandardScaler()\n", "X_train_scaled = scaler.fit_transform(X_train)\n", "X_test_scaled = scaler.transform(X_test)\n", "\n", "models = {\n", "    'LinearRegression': LinearRegression(),\n", "    'Ridge': Ridge(random_state=42),\n", "    'Lasso': Lasso(random_state=42),\n", "    'DecisionTree': DecisionTreeRegressor(random_state=42),\n", "    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n", "    'KNN': KNeighborsRegressor(),\n", "    'GradientBoosting': GradientBoostingRegressor(random_state=42),\n", "    'SVR': SVR()\n", "}\n", "metrics = {}\n", "preds = {}\n", "for name, model in models.items():\n", "    if name in ['SVR','KNN']:\n", "        model.fit(X_train_scaled, y_train)\n", "        p = model.predict(X_test_scaled)\n", "    else:\n", "        model.fit(X_train, y_train)\n", "        p = model.predict(X_test)\n", "    preds[name] = p\n", "    mse = mean_squared_error(y_test, p)\n", "    mae = mean_absolute_error(y_test, p)\n", "    r2 = r2_score(y_test, p)\n", "    metrics[name] = {'MSE': mse, 'MAE': mae, 'RMSE': np.sqrt(mse), 'R2': r2}\n", "\n", "metrics_df = pd.DataFrame(metrics).T.sort_values('RMSE')\n", "display(metrics_df)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Model Comparison \u2014 Combined Plots (bar, line, box, ROC-like)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_names = list(metrics.keys())\n", "r2_vals = [metrics[m]['R2'] for m in model_names]\n", "rmse_vals = [metrics[m]['RMSE'] for m in model_names]\n", "\n", "abs_err_list = []\n", "for m in model_names:\n", "    ae = np.abs(y_test - preds[m])\n", "    tmp = pd.DataFrame({'Model': m, 'AbsError': ae})\n", "    abs_err_list.append(tmp)\n", "abs_err_df = pd.concat(abs_err_list, ignore_index=True)\n", "\n", "threshold = y_test.mean()\n", "fig, axes = plt.subplots(2,2, figsize=(16,12))\n", "sns.barplot(x=r2_vals, y=model_names, ax=axes[0,0])\n", "axes[0,0].set_title('R2 by Model')\n", "\n", "sns.lineplot(x=model_names, y=rmse_vals, marker='o', ax=axes[0,1])\n", "axes[0,1].set_title('RMSE by Model')\n", "axes[0,1].set_xticklabels(model_names, rotation=45)\n", "\n", "sns.boxplot(x='Model', y='AbsError', data=abs_err_df, ax=axes[1,0])\n", "axes[1,0].set_title('Absolute Error Distribution')\n", "axes[1,0].tick_params(axis='x', rotation=45)\n", "\n", "for m in model_names:\n", "    fpr, tpr, _ = roc_curve(y_test >= threshold, preds[m] >= threshold)\n", "    roc_auc = auc(fpr, tpr)\n", "    axes[1,1].plot(fpr, tpr, label=f\"{m} (AUC={roc_auc:.2f})\")\n", "axes[1,1].plot([0,1],[0,1],'k--')\n", "axes[1,1].set_title('ROC-like Curves (pass = score >= mean)')\n", "axes[1,1].set_xlabel('FPR')\n", "axes[1,1].set_ylabel('TPR')\n", "axes[1,1].legend(loc='lower right')\n", "\n", "plt.suptitle('Model Comparison - 4 Plots Combined', fontsize=16)\n", "plt.tight_layout(rect=[0,0,1,0.96])\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Optional: save the best model\n", "Uncomment and run if you want to save the best model pipeline locally."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# import joblib\n", "# best = metrics_df.index[0]\n", "# joblib.dump({'model': models[best], 'scaler': scaler}, 'best_model_pipeline.joblib')\n", "# print('Saved best_model_pipeline.joblib')\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}