{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8f6bba",
   "metadata": {},
   "source": [
    "# Hull Tactical Market Prediction - Exploratory Notebook\n",
    "\n",
    "This notebook loads the provided `train.csv` and `test.csv` for the Hull Tactical Market Prediction-style dataset, performs EDA, feature engineering, trains baseline XGBoost and a small Keras neural network, performs hyperparameter tuning, and simulates a trading strategy that converts predictions into portfolio weights (0-2) and evaluates performance using a Sharpe-like metric that penalizes excess volatility.\n",
    "\n",
    "Notes: The target column is `market_forward_excess_returns` (daily excess returns versus risk free). The notebook is designed to run in a Kaggle-like environment (Python 3.10+, Pandas, NumPy, scikit-learn, XGBoost, TensorFlow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c581d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# Add these imports in the first cell\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "# OR simply\n",
    "# Print versions for reproducibility\n",
    "print('pandas', pd.__version__)\n",
    "print('numpy', np.__version__)\n",
    "import xgboost as xgb\n",
    "print('xgboost', xgb.__version__)\n",
    "print('tensorflow', tf.__version__)\n",
    "print('scikit-learn', __import__('sklearn').__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f846eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/train.csv')\n",
    "print('train shape', train.shape)\n",
    "display(train.head())\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/hull-tactical-market-prediction/test.csv')\n",
    "print('test shape', test.shape)\n",
    "test.head()\n",
    "\n",
    "print('test.csv and train.csv file found ',)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b012ef",
   "metadata": {},
   "source": [
    "## Basic inspection and target identification\n",
    "We confirm the target `market_forward_excess_returns` and inspect missing values and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f770c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'market_forward_excess_returns'\n",
    "print('target in columns?', target in train.columns)\n",
    "display(train.dtypes.value_counts())\n",
    "missing = train.isna().sum().sort_values(ascending=False)\n",
    "display(missing[missing>0].head(30))\n",
    "display(train[[target, 'forward_returns', 'risk_free_rate']].describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24957c3e",
   "metadata": {},
   "source": [
    "## Visualizations: target series, histogram, boxplot, correlation heatmap\n",
    "We plot the target series and distributions to understand its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb0308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(train['date_id'], train[target], label='market_forward_excess_returns')\n",
    "plt.xlabel('date_id')\n",
    "plt.ylabel('excess return')\n",
    "plt.title('Target series over date_id')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,4))\n",
    "sns.histplot(train[target].dropna(), bins=80, ax=axes[0], kde=True)\n",
    "axes[0].set_title('Histogram of target')\n",
    "sns.boxplot(x=train[target].dropna(), ax=axes[1])\n",
    "axes[1].set_title('Boxplot of target')\n",
    "plt.show()\n",
    "corr = train.corr()\n",
    "corr_target = corr[target].abs().sort_values(ascending=False)\n",
    "top_feats = corr_target.index[1:31].tolist()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(train[top_feats + [target]].corr(), cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
    "plt.title('Correlation matrix - top features vs target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8263499",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Create lag features, rolling means and rolling volatility for `forward_returns` and the target where appropriate. We'll drop near-constant features and optionally use PCA for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e30ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.copy()\n",
    "for lag in [1,2,3]:\n",
    "    df[f'forward_returns_lag{lag}'] = df['forward_returns'].shift(lag)\n",
    "    df[f'{target}_lag{lag}'] = df[target].shift(lag)\n",
    "df['fr_roll_mean_5'] = df['forward_returns'].rolling(window=5, min_periods=1).mean()\n",
    "df['fr_roll_std_5'] = df['forward_returns'].rolling(window=5, min_periods=1).std().fillna(0)\n",
    "df['fr_roll_mean_20'] = df['forward_returns'].rolling(window=20, min_periods=1).mean()\n",
    "df['fr_roll_std_20'] = df['forward_returns'].rolling(window=20, min_periods=1).std().fillna(0)\n",
    "nunique = df.nunique()\n",
    "constant_cols = nunique[nunique<=1].index.tolist()\n",
    "print('constant or single-value columns:', len(constant_cols))\n",
    "df.drop(columns=constant_cols, inplace=True, errors='ignore')\n",
    "df.fillna(0, inplace=True)\n",
    "print('df shape after feature engineering', df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cac769",
   "metadata": {},
   "source": [
    "## Prepare training and validation splits (time-aware)\n",
    "We'll use `date_id` to split the data in time order to avoid lookahead bias. Use the last 20% of time as validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a98c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in df.columns if c not in ['date_id', target, 'forward_returns', 'risk_free_rate']]\n",
    "print('selected feature count', len(features))\n",
    "date_cut = df['date_id'].quantile(0.8)\n",
    "train_idx = df['date_id'] <= date_cut\n",
    "val_idx = df['date_id'] > date_cut\n",
    "X_train = df.loc[train_idx, features].values\n",
    "y_train = df.loc[train_idx, target].values\n",
    "X_val = df.loc[val_idx, features].values\n",
    "y_val = df.loc[val_idx, target].values\n",
    "print('X_train', X_train.shape, 'X_val', X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036765b5",
   "metadata": {},
   "source": [
    "## Baseline XGBoost model\n",
    "Train a baseline XGBRegressor and evaluate on the time-validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42, n_jobs=4)\n",
    "xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=10, verbose=False)\n",
    "pred_val = xgb_model.predict(X_val)\n",
    "print('Baseline XGBoost - MSE', mean_squared_error(y_val, pred_val))\n",
    "print('Baseline XGBoost - MAE', mean_absolute_error(y_val, pred_val))\n",
    "print('Baseline XGBoost - R2', r2_score(y_val, pred_val))\n",
    "try:\n",
    "    fi = xgb_model.get_booster().get_score(importance_type='gain')\n",
    "    fi2 = {features[int(k.replace('f',''))]:v for k,v in fi.items()}\n",
    "    sorted_fi = sorted(fi2.items(), key=lambda x: x[1], reverse=True)\n",
    "    print('Top 15 features by gain')\n",
    "    for f,v in sorted_fi[:15]:\n",
    "        print(f, v)\n",
    "except Exception as e:\n",
    "    print('feature importance error', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e848e3",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning (full run)\n",
    "Perform a GridSearchCV using TimeSeriesSplit to find reasonable XGBoost params. This grid is larger and will run longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d8de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "param_grid = {\n",
    "    'max_depth': [3,5,7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "gsearch = GridSearchCV(estimator=XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=4),\n",
    "                   param_grid=param_grid, cv=tscv, scoring='neg_mean_squared_error', verbose=2)\n",
    "gsearch.fit(X_train, y_train)\n",
    "print('best params', gsearch.best_params_)\n",
    "best_xgb = gsearch.best_estimator_\n",
    "pred_val_g = best_xgb.predict(X_val)\n",
    "print('Tuned XGBoost - MSE', mean_squared_error(y_val, pred_val_g))\n",
    "print('Tuned XGBoost - MAE', mean_absolute_error(y_val, pred_val_g))\n",
    "print('Tuned XGBoost - R2', r2_score(y_val, pred_val_g))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dbe394",
   "metadata": {},
   "source": [
    "## Keras feedforward network (full train)\n",
    "Build and train a dense neural network with early stopping (up to 100 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f3ad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s = scaler.transform(X_val)\n",
    "def build_ffn(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_shape,)),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "ffn = build_ffn(X_train_s.shape[1])\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = ffn.fit(X_train_s, y_train, validation_data=(X_val_s, y_val), epochs=100, batch_size=128, callbacks=[es], verbose=2)\n",
    "pred_val_nn = ffn.predict(X_val_s).ravel()\n",
    "print('NN - MSE', mean_squared_error(y_val, pred_val_nn))\n",
    "print('NN - MAE', mean_absolute_error(y_val, pred_val_nn))\n",
    "print('NN - R2', r2_score(y_val, pred_val_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c093446",
   "metadata": {},
   "source": [
    "## Strategy Simulation & Competition-style Scoring\n",
    "Convert predictions to portfolio weights in [0,2], apply a 120% volatility cap relative to market volatility, include transaction costs, and compute an adjusted Sharpe ratio similar to the competition metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45bbd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tuned XGBoost predictions as primary signal\n",
    "signal = pred_val_g\n",
    "def signal_to_weight(s, lower=0.0, upper=2.0):\n",
    "    lo = np.percentile(s, 5)\n",
    "    hi = np.percentile(s, 95)\n",
    "    w = (s - lo) / (hi - lo + 1e-9) * (upper - lower) + lower\n",
    "    return np.clip(w, lower, upper)\n",
    "weights = signal_to_weight(signal)\n",
    "val_df = df.loc[val_idx].copy().reset_index(drop=True)\n",
    "val_df['pred'] = signal\n",
    "val_df['weight'] = weights\n",
    "# transaction cost per round-trip (default 5 bps)\n",
    "tc = 0.0005\n",
    "# compute turnover as abs change in weight (day-over-day) and apply cost\n",
    "val_df['turnover'] = val_df['weight'].diff().abs().fillna(0)\n",
    "val_df['tc_cost'] = val_df['turnover'] * tc\n",
    "# strategy excess returns before cost\n",
    "val_df['strategy_excess'] = val_df['weight'] * val_df['forward_returns']\n",
    "# subtract transaction cost from strategy returns\n",
    "val_df['strategy_excess_net'] = val_df['strategy_excess'] - val_df['tc_cost']\n",
    "# Sharpe function\n",
    "def sharpe_ratio(returns, periods=252):\n",
    "    mean = returns.mean()\n",
    "    vol = returns.std()\n",
    "    if vol == 0: return np.nan\n",
    "    return (mean/vol) * np.sqrt(periods)\n",
    "strat_ret = val_df['strategy_excess_net']\n",
    "market_ret = val_df['forward_returns']\n",
    "print('Strategy net mean daily', strat_ret.mean())\n",
    "print('Market mean daily', market_ret.mean())\n",
    "print('Strategy net Sharpe', sharpe_ratio(strat_ret))\n",
    "print('Market Sharpe', sharpe_ratio(market_ret))\n",
    "# volatility cap: scale weights to keep strategy vol <= 1.2 * market vol\n",
    "market_vol = market_ret.std()\n",
    "strat_vol = strat_ret.std()\n",
    "cap = 1.2 * market_vol\n",
    "print('market vol', market_vol, 'strategy vol', strat_vol, 'cap', cap)\n",
    "if strat_vol > cap and strat_vol>0:\n",
    "    scale = cap / strat_vol\n",
    "    val_df['weight_adj'] = val_df['weight'] * scale\n",
    "    val_df['turnover_adj'] = val_df['weight_adj'].diff().abs().fillna(0)\n",
    "    val_df['tc_cost_adj'] = val_df['turnover_adj'] * tc\n",
    "    val_df['strategy_excess_adj'] = val_df['weight_adj'] * val_df['forward_returns'] - val_df['tc_cost_adj']\n",
    "    print('Adjusted Strategy Sharpe', sharpe_ratio(val_df['strategy_excess_adj']))\n",
    "else:\n",
    "    val_df['weight_adj'] = val_df['weight']\n",
    "    val_df['strategy_excess_adj'] = val_df['strategy_excess_net']\n",
    "# cumulative returns\n",
    "val_df['cum_strategy'] = (1 + val_df['strategy_excess_adj']).cumprod() - 1\n",
    "val_df['cum_market'] = (1 + val_df['forward_returns']).cumprod() - 1\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(val_df['date_id'], val_df['cum_strategy'], label='Strategy (net, adj)')\n",
    "plt.plot(val_df['date_id'], val_df['cum_market'], label='Market')\n",
    "plt.legend()\n",
    "plt.title('Cumulative returns: strategy vs market (validation)')\n",
    "plt.show()\n",
    "display(val_df[['date_id','pred','weight','weight_adj','forward_returns','strategy_excess_adj']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe3b15",
   "metadata": {},
   "source": [
    "## Discussion & Next steps\n",
    "- The notebook shows a complete flow from data loading to strategy simulation.\n",
    "- Next improvements: cross-sectional signals (if available), ensemble models, rolling retraining, transaction cost models, stricter backtest hygiene to avoid data leakage, and using PCA/feature selection for correlated features.\n",
    "- Be mindful that beating the S&P on historical data does not guarantee future performance and may reflect data snooping.\n",
    "\n",
    "If you'd like, I can re-run this notebook now (full run will take time), or run a quicker version first."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
