{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ae0caf",
   "metadata": {},
   "source": [
    "# üõí Market Basket Intelligence: From Groceries to Great Recommendations\n",
    "\n",
    "> **Imagine** you're a data scientist at a large retail chain. Every day, thousands of customers buy items together ‚Äî\n",
    "> tea with biscuits, pasta with sauce, or maybe chocolate with everything. Hidden in those baskets are **patterns** \n",
    "> that can make recommendations smarter, shelves better arranged, and promotions more profitable.  \n",
    ">\n",
    "> In this notebook, we‚Äôll turn a raw transaction log into **actionable insights** using Market Basket Analysis, \n",
    "> uncovering which products **love to be bought together** and how that knowledge can power **recommendation systems**.\n",
    ">\n",
    "> **Dataset**: `Market_Basket_Optimisation.csv` ‚Äî a classic transactional dataset where each row represents a single\n",
    "> shopping basket with items spread across columns.\n",
    ">\n",
    "> **Problem Type**: Unsupervised learning ‚Üí **Association Rule Mining** (Apriori / FP-Growth) + simple **recommender evaluation**.\n",
    ">\n",
    "> **Main Questions**  \n",
    "> 1. Which items are most frequently purchased?  \n",
    "> 2. What item combinations (pairs/triads) commonly occur together?  \n",
    "> 3. Which association rules (X ‚áí Y) are the strongest by **support**, **confidence**, and **lift**?  \n",
    "> 4. Can these rules help **recommend** the next item in a basket?\n",
    ">\n",
    "> ### ‚ú® What You‚Äôll See\n",
    "> - Clean, reproducible EDA with clear visualizations.  \n",
    "> - Association rules through Apriori (with a built‚Äëin fallback if libraries aren‚Äôt available).  \n",
    "> - Side‚Äëby‚Äëside comparison of algorithms and rule quality.  \n",
    "> - Lightweight evaluation of recommendations using hold‚Äëout baskets.\n",
    ">\n",
    "> Let‚Äôs dive in! üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dede9ec",
   "metadata": {},
   "source": [
    "## 1. Introduction / Problem Statement ‚úçÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3309b442",
   "metadata": {},
   "source": [
    "## 2. Import Libraries + Basic Setup üß∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Matplotlib defaults (keep simple and clean; no explicit colors/styles)\n",
    "plt.rcParams['figure.figsize'] = (9, 5)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "DATA_PATH = \"/mnt/data/Market_Basket_Optimisation.csv\"\n",
    "assert os.path.exists(DATA_PATH), f\"Dataset not found at {DATA_PATH}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d75b6a",
   "metadata": {},
   "source": [
    "## 3. Dataset Loading + First Look üëÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd850b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df_raw = pd.read_csv(DATA_PATH, header=None)\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cf8f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic overview\n",
    "print(\"Shape:\", df_raw.shape)\n",
    "print(\"\\nInfo:\")\n",
    "df_raw.info()\n",
    "print(\"\\nDescribe (non-numeric will be skipped or shown as object count):\")\n",
    "display(df_raw.describe(include='all'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2dbf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values & duplicates check\n",
    "missing_count = df_raw.isna().sum().sum()\n",
    "print(\"Total missing values:\", missing_count)\n",
    "\n",
    "# Each row is a basket; duplicates at row-level are acceptable in transactional logs,\n",
    "# but we still check exact duplicate rows:\n",
    "dupe_rows = df_raw.duplicated().sum()\n",
    "print(\"Duplicate rows:\", dupe_rows)\n",
    "\n",
    "# Transform to list-of-lists (transactions), dropping NaNs/empties, and trimming whitespace\n",
    "transactions = []\n",
    "for _, row in df_raw.iterrows():\n",
    "    items = [str(x).strip() for x in row.values if isinstance(x, str) and str(x).strip() != \"\" and str(x).lower() != \"nan\"]\n",
    "    if items:\n",
    "        transactions.append(items)\n",
    "\n",
    "print(\"Total transactions (non-empty):\", len(transactions))\n",
    "\n",
    "# Memory usage (rough estimate via DataFrame)\n",
    "mem_bytes = df_raw.memory_usage(index=True).sum()\n",
    "print(f\"Approx memory usage: {mem_bytes/1024:.2f} KB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0878f3",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA) üîé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item frequencies\n",
    "all_items = list(itertools.chain.from_iterable(transactions))\n",
    "item_counts = Counter(all_items)\n",
    "item_freq = pd.DataFrame(item_counts.items(), columns=[\"item\", \"count\"]).sort_values(\"count\", ascending=False)\n",
    "item_freq.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a7d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Top 20 items\n",
    "top_k = 20\n",
    "top_items = item_freq.head(top_k)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(top_items[\"item\"], top_items[\"count\"])\n",
    "plt.xticks(rotation=75, ha='right')\n",
    "plt.title(f\"Top {top_k} Most Frequent Items\")\n",
    "plt.xlabel(\"Item\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"These are the most frequently purchased items across all baskets. Higher bars indicate items that appear in more transactions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e90204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basket length distribution\n",
    "basket_lengths = [len(t) for t in transactions]\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(basket_lengths, bins=range(1, max(basket_lengths)+2))\n",
    "plt.title(\"Basket Size Distribution\")\n",
    "plt.xlabel(\"Number of items in a basket\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Most baskets contain a small number of items. This distribution helps set expectations for rule sizes and support thresholds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e51de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique items and basic stats\n",
    "n_transactions = len(transactions)\n",
    "n_unique_items = item_freq.shape[0]\n",
    "avg_basket_size = np.mean(basket_lengths)\n",
    "\n",
    "print(f\"Unique items: {n_unique_items}\")\n",
    "print(f\"Average basket size: {avg_basket_size:.2f}\")\n",
    "print(f\"Median basket size: {np.median(basket_lengths):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee311e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair (2-item) co-occurrence counts (top 20)\n",
    "pair_counts = Counter()\n",
    "for t in transactions:\n",
    "    for a, b in itertools.combinations(sorted(set(t)), 2):\n",
    "        pair_counts[(a, b)] += 1\n",
    "\n",
    "pair_df = pd.DataFrame([{\"item_a\": a, \"item_b\": b, \"count\": c} for (a, b), c in pair_counts.items()])\n",
    "pair_df = pair_df.sort_values(\"count\", ascending=False).head(20)\n",
    "pair_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8afac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Top 20 item pairs by co-occurrence\n",
    "plt.figure()\n",
    "plt.bar([f\"{a} & {b}\" for a,b in zip(pair_df[\"item_a\"], pair_df[\"item_b\"])], pair_df[\"count\"])\n",
    "plt.xticks(rotation=75, ha='right')\n",
    "plt.title(\"Top 20 Item Pairs by Co-occurrence\")\n",
    "plt.xlabel(\"Item Pair\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"These item pairs co-occur most often in the same basket. They are candidates for strong association rules.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339336f",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning / Preprocessing üßπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b103a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode transactions for association mining\n",
    "# Build item index\n",
    "items_sorted = sorted(item_freq['item'].tolist())\n",
    "item_to_idx = {item:i for i, item in enumerate(items_sorted)}\n",
    "\n",
    "# Sparse-like construction into a DataFrame\n",
    "rows = []\n",
    "for t in transactions:\n",
    "    row = [0]*len(items_sorted)\n",
    "    for it in set(t):  # set to avoid double count within a basket\n",
    "        row[item_to_idx[it]] = 1\n",
    "    rows.append(row)\n",
    "\n",
    "basket_ohe = pd.DataFrame(rows, columns=items_sorted)\n",
    "basket_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple feature engineering ideas (illustrative, not always needed for ARM):\n",
    "# 1) Basket size as a feature (for recommendation diagnostics)\n",
    "# 2) Weekend vs weekday purchases (if timestamps existed; here we don't have them)\n",
    "# We'll keep basket size vector for later evaluation slices.\n",
    "basket_sizes = pd.Series(basket_lengths, name=\"basket_size\")\n",
    "basket_sizes.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1b5b6c",
   "metadata": {},
   "source": [
    "## 6. Model Building (Baseline + Advanced) üß†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacc4d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will mine frequent itemsets and generate association rules.\n",
    "# Try mlxtend if available; otherwise use a minimal Apriori fallback.\n",
    "\n",
    "def generate_rules_from_itemsets(freq_itemsets, min_confidence=0.2):\n",
    "    \"\"\"Given dict itemset->support_count and total transaction count,\n",
    "    compute rules with confidence & lift.\"\"\"\n",
    "    n_trans = len(transactions)\n",
    "    support = {itemset:cnt/n_trans for itemset, cnt in freq_itemsets.items()}\n",
    "    rules = []\n",
    "    for itemset, cnt in freq_itemsets.items():\n",
    "        if len(itemset) < 2:\n",
    "            continue\n",
    "        items = list(itemset)\n",
    "        for r in range(1, len(items)):\n",
    "            for antecedent in itertools.combinations(items, r):\n",
    "                consequent = tuple(sorted(set(items) - set(antecedent)))\n",
    "                if not consequent:\n",
    "                    continue\n",
    "                antecedent = tuple(sorted(antecedent))\n",
    "                sup_itemset = support[itemset]\n",
    "                sup_ante = support.get(antecedent, 0)\n",
    "                sup_cons = support.get(consequent, 0)\n",
    "                if sup_ante == 0 or sup_cons == 0:\n",
    "                    continue\n",
    "                conf = sup_itemset / sup_ante\n",
    "                lift = conf / sup_cons if sup_cons > 0 else np.nan\n",
    "                if conf >= min_confidence:\n",
    "                    rules.append({\n",
    "                        \"antecedent\": antecedent,\n",
    "                        \"consequent\": consequent,\n",
    "                        \"support\": sup_itemset,\n",
    "                        \"confidence\": conf,\n",
    "                        \"lift\": lift\n",
    "                    })\n",
    "    return pd.DataFrame(rules).sort_values([\"confidence\",\"lift\",\"support\"], ascending=False)\n",
    "\n",
    "def apriori_fallback(transactions, min_support=0.01, max_len=3):\n",
    "    n_trans = len(transactions)\n",
    "    # L1\n",
    "    item_counts = Counter(itertools.chain.from_iterable(transactions))\n",
    "    L = { (i,): c for i,c in item_counts.items() if c/n_trans >= min_support }\n",
    "    freq_itemsets = dict(L)\n",
    "    k = 2\n",
    "    while L and k <= max_len:\n",
    "        # candidate generation\n",
    "        items = sorted(set(itertools.chain.from_iterable(L.keys())))\n",
    "        Ck = set()\n",
    "        L_keys = list(L.keys())\n",
    "        for i in range(len(L_keys)):\n",
    "            for j in range(i+1, len(L_keys)):\n",
    "                a, b = L_keys[i], L_keys[j]\n",
    "                union = tuple(sorted(set(a) | set(b)))\n",
    "                if len(union) == k:\n",
    "                    # prune: all (k-1)-subsets must be frequent\n",
    "                    all_subs_ok = True\n",
    "                    for sub in itertools.combinations(union, k-1):\n",
    "                        if sub not in L:\n",
    "                            all_subs_ok = False\n",
    "                            break\n",
    "                    if all_subs_ok:\n",
    "                        Ck.add(union)\n",
    "        # count candidates\n",
    "        Ck_counts = Counter()\n",
    "        for t in transactions:\n",
    "            tset = set(t)\n",
    "            for cand in Ck:\n",
    "                if set(cand).issubset(tset):\n",
    "                    Ck_counts[cand] += 1\n",
    "        # filter by support\n",
    "        L = { cand:cnt for cand,cnt in Ck_counts.items() if cnt/n_trans >= min_support }\n",
    "        freq_itemsets.update(L)\n",
    "        k += 1\n",
    "    return freq_itemsets\n",
    "\n",
    "# Try mlxtend\n",
    "use_mlxtend = False\n",
    "try:\n",
    "    from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth\n",
    "    use_mlxtend = True\n",
    "except Exception as e:\n",
    "    print(\"mlxtend not available; using fallback Apriori.\", str(e))\n",
    "\n",
    "results = {}\n",
    "\n",
    "if use_mlxtend:\n",
    "    # Apriori\n",
    "    t0 = time.time()\n",
    "    fi_ap = apriori(basket_ohe.astype(bool), min_support=0.01, use_colnames=True, max_len=3)\n",
    "    fi_ap[\"length\"] = fi_ap[\"itemsets\"].apply(len)\n",
    "    rules_ap = association_rules(fi_ap, metric=\"confidence\", min_threshold=0.2)\n",
    "    t1 = time.time()\n",
    "    results['Apriori'] = {\n",
    "        \"fi\": fi_ap.sort_values(\"support\", ascending=False),\n",
    "        \"rules\": rules_ap.sort_values([\"confidence\",\"lift\",\"support\"], ascending=False),\n",
    "        \"time\": t1 - t0\n",
    "    }\n",
    "\n",
    "    # FP-Growth\n",
    "    t0 = time.time()\n",
    "    fi_fp = fpgrowth(basket_ohe.astype(bool), min_support=0.01, use_colnames=True, max_len=3)\n",
    "    fi_fp[\"length\"] = fi_fp[\"itemsets\"].apply(len)\n",
    "    rules_fp = association_rules(fi_fp, metric=\"confidence\", min_threshold=0.2)\n",
    "    t1 = time.time()\n",
    "    results['FP-Growth'] = {\n",
    "        \"fi\": fi_fp.sort_values(\"support\", ascending=False),\n",
    "        \"rules\": rules_fp.sort_values([\"confidence\",\"lift\",\"support\"], ascending=False),\n",
    "        \"time\": t1 - t0\n",
    "    }\n",
    "else:\n",
    "    # Fallback Apriori only\n",
    "    t0 = time.time()\n",
    "    fi_dict = apriori_fallback(transactions, min_support=0.01, max_len=3)\n",
    "    t1 = time.time()\n",
    "    fi_df = pd.DataFrame(\n",
    "        [ {\"itemsets\": set(k), \"support\": v/len(transactions), \"length\": len(k)} for k,v in fi_dict.items() ]\n",
    "    ).sort_values(\"support\", ascending=False)\n",
    "    rules_ap = generate_rules_from_itemsets(fi_dict, min_confidence=0.2)\n",
    "    results['Apriori (fallback)'] = {\n",
    "        \"fi\": fi_df,\n",
    "        \"rules\": rules_ap,\n",
    "        \"time\": t1 - t0\n",
    "    }\n",
    "\n",
    "# Peek rules\n",
    "for k,v in results.items():\n",
    "    print(f\"\\n{k}: {len(v['fi'])} frequent itemsets, {len(v['rules'])} rules, time: {v['time']:.3f}s\")\n",
    "    display(v['rules'].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5497e4cc",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation üìè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7687fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For association rules, classic metrics are support, confidence, and lift.\n",
    "# We'll also build a tiny next-item recommendation test to compare algorithms.\n",
    "# Protocol: For each basket with length >= 2, hide the last item as a 'target'.\n",
    "# Use rules to recommend top-K consequents based on items in the observed prefix.\n",
    "# Compute Precision@K and HitRate@K.\n",
    "\n",
    "def build_recommender_from_rules(rules_df, topn=3):\n",
    "    \"\"\"Create a mapping from antecedent -> list of (consequent, score) sorted by confidence*lift.\"\"\"\n",
    "    mapping = defaultdict(list)\n",
    "    if rules_df is None or rules_df.empty:\n",
    "        return mapping\n",
    "    # Normalize format between mlxtend and fallback\n",
    "    if 'antecedents' in rules_df.columns:\n",
    "        # mlxtend format\n",
    "        for _, row in rules_df.iterrows():\n",
    "            ante = tuple(sorted(list(row['antecedents'])))\n",
    "            cons = tuple(sorted(list(row['consequents'])))\n",
    "            score = float(row.get('confidence', 0)) * float(row.get('lift', 1))\n",
    "            mapping[ante].append((cons, score))\n",
    "    else:\n",
    "        for _, row in rules_df.iterrows():\n",
    "            ante = tuple(sorted(list(row['antecedent'])))\n",
    "            cons = tuple(sorted(list(row['consequent'])))\n",
    "            score = float(row.get('confidence', 0)) * float(row.get('lift', 1))\n",
    "            mapping[ante].append((cons, score))\n",
    "\n",
    "    for k in mapping:\n",
    "        mapping[k] = sorted(mapping[k], key=lambda x: x[1], reverse=True)[:topn]\n",
    "    return mapping\n",
    "\n",
    "def recommend(prefix_items, rule_map, K=5):\n",
    "    \"\"\"Aggregate recommendations from all rule antecedents that are subset of prefix.\"\"\"\n",
    "    prefix_set = set(prefix_items)\n",
    "    cand_scores = defaultdict(float)\n",
    "    for ante, cons_list in rule_map.items():\n",
    "        if set(ante).issubset(prefix_set):\n",
    "            for cons, score in cons_list:\n",
    "                for item in cons:\n",
    "                    if item not in prefix_set:\n",
    "                        cand_scores[item] += score\n",
    "    ranked = sorted(cand_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [it for it,_ in ranked[:K]]\n",
    "\n",
    "# Build train/test split by baskets\n",
    "rng = np.random.default_rng(42)\n",
    "indices = np.arange(len(transactions))\n",
    "rng.shuffle(indices)\n",
    "split = int(0.8 * len(indices))\n",
    "train_idx, test_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_tx = [transactions[i] for i in train_idx]\n",
    "test_tx  = [transactions[i] for i in test_idx]\n",
    "\n",
    "# Re-mine on train only (simple)\n",
    "# To keep it light, reuse earlier frequent itemsets/rules mined on all data,\n",
    "# but for a stricter protocol you could re-run mining on train_tx.\n",
    "\n",
    "def evaluate_rules(rule_df, test_baskets, K=5):\n",
    "    rule_map = build_recommender_from_rules(rule_df, topn=5)\n",
    "    hits = 0\n",
    "    total = 0\n",
    "    all_prec = []\n",
    "    for t in test_baskets:\n",
    "        if len(t) < 2:\n",
    "            continue\n",
    "        prefix = t[:-1]\n",
    "        target = t[-1]\n",
    "        recs = recommend(prefix, rule_map, K=K)\n",
    "        if not recs:\n",
    "            continue\n",
    "        total += 1\n",
    "        hit = 1 if target in recs else 0\n",
    "        hits += hit\n",
    "        prec = hit / len(recs)\n",
    "        all_prec.append(prec)\n",
    "    hit_rate = hits / total if total > 0 else 0.0\n",
    "    mean_precision = float(np.mean(all_prec)) if all_prec else 0.0\n",
    "    return hit_rate, mean_precision, total\n",
    "\n",
    "comparison_rows = []\n",
    "for name, out in results.items():\n",
    "    rules_df = out['rules']\n",
    "    hr, mp, n_eval = evaluate_rules(rules_df, test_tx, K=5)\n",
    "    comparison_rows.append({\n",
    "        \"Model\": name,\n",
    "        \"Rules\": len(rules_df) if rules_df is not None else 0,\n",
    "        \"Time (s)\": round(out['time'], 3),\n",
    "        \"HitRate@5\": round(hr, 4),\n",
    "        \"MeanPrecision@5\": round(mp, 4),\n",
    "        \"Eval Baskets\": n_eval\n",
    "    })\n",
    "\n",
    "model_comparison = pd.DataFrame(comparison_rows).sort_values(\"HitRate@5\", ascending=False)\n",
    "model_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model performance\n",
    "plt.figure()\n",
    "plt.bar(model_comparison['Model'], model_comparison['HitRate@5'])\n",
    "plt.title(\"Model HitRate@5\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"HitRate@5\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(model_comparison['Model'], model_comparison['MeanPrecision@5'])\n",
    "plt.title(\"Model MeanPrecision@5\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"MeanPrecision@5\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"We evaluate recommendation quality using hold-out baskets. HitRate@5 measures how often the hidden last item appears in the top-5 recommendations. MeanPrecision@5 penalizes longer lists if they don't include the target.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97fe69f",
   "metadata": {},
   "source": [
    "## 9. Feature Importance + Explainability üí°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e32aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For association rules, 'explainability' comes from rule metrics:\n",
    "# - Support: how common the itemset is\n",
    "# - Confidence: P(Y | X) likelihood the consequent occurs given the antecedent\n",
    "# - Lift: multiplicative boost over random chance; >1 is positive association\n",
    "\n",
    "# Show top rules by lift for the best model\n",
    "best_model = model_comparison.iloc[0]['Model']\n",
    "best_rules = results[best_model]['rules']\n",
    "\n",
    "# Normalize columns for display\n",
    "if 'antecedents' in best_rules.columns:\n",
    "    disp_rules = best_rules[['antecedents','consequents','support','confidence','lift']].head(10)\n",
    "else:\n",
    "    disp_rules = best_rules[['antecedent','consequent','support','confidence','lift']].head(10)\n",
    "\n",
    "disp_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c218a50",
   "metadata": {},
   "source": [
    "## 10. Conclusion + Future Work ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4510b",
   "metadata": {},
   "source": [
    "- We explored a classic market-basket dataset and identified the most frequent items and pairs.  \n",
    "- Using **association rule mining**, we derived interpretable rules (X ‚áí Y) with strong **support**, **confidence**, and **lift**.  \n",
    "- A light-weight recommendation evaluation showed which algorithm produced more useful rules for next-item prediction.  \n",
    "\n",
    "**Future Work**  \n",
    "- Incorporate timestamps and customer IDs (if available) to build **personalized** and **temporal** recommenders.  \n",
    "- Explore **higher-order** itemsets (len ‚â• 4) with careful pruning for efficiency.  \n",
    "- Tune thresholds (min support/confidence) and try **advanced recommenders** (e.g., Bayesian Personalized Ranking, sequence models).  \n",
    "- Deploy rules into a real-time system for **in-cart recommendations**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e733d7",
   "metadata": {},
   "source": [
    "## 11. About the Author / Further Resources üßë‚Äçüíª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac14700",
   "metadata": {},
   "source": [
    "Hi, I'm **Taha Ali** ‚Äî passionate about data science and practical AI.  \n",
    "If you found this helpful, feel free to connect and explore more work:\n",
    "\n",
    "- LinkedIn: *add your link here*  \n",
    "- GitHub: *add your link here*  \n",
    "- Kaggle: *add your link here*  \n",
    "\n",
    "> Tip: Replace these placeholders with your real links to make the notebook portfolio-ready.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
