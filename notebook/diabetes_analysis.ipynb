{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5c76e9",
   "metadata": {},
   "source": [
    "# Diabetes Dataset Analysis & ML Model\n",
    "\n",
    "This notebook **loads the provided `diabetes_dataset.csv`**, performs EDA, preprocessing, trains a couple of ML models (Logistic Regression & RandomForest), and includes **interactive plots** (Plotly) for exploration. Designed for quick copy-run in your environment. \n",
    "\n",
    "**File path used:** `/mnt/data/diabetes_dataset.csv`\n",
    "\n",
    "**Generated:** 2025-08-11 14:08:57 UTC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6182d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports - assume these are already installed as you said\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# scikit-learn for modeling\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, classification_report\n",
    "\n",
    "# Plotly for interactive plots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# For nicer warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Imports done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c98fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "path = '/mnt/data/diabetes_dataset.csv'\n",
    "df = pd.read_csv(path)\n",
    "print('Dataset shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd60893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick info\n",
    "display(df.info())\n",
    "display(df.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d288bb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values and zeros that may represent missing\n",
    "missing = df.isnull().sum()\n",
    "zeros = (df == 0).sum()\n",
    "print('Missing values:\\n', missing)\n",
    "print('\\nZero counts (may be missing for some columns):\\n', zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99d94c7",
   "metadata": {},
   "source": [
    "**Note:** In many diabetes datasets, columns like `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI` contain zeros which actually mean missing. We'll replace zeros with NaN for these columns and impute with median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4677cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zeros with NaN for selected columns then impute with median\n",
    "cols_zero_missing = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\n",
    "for c in cols_zero_missing:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].replace(0, np.nan)\n",
    "\n",
    "# show missing after replacement\n",
    "display(df[cols_zero_missing].isnull().sum())\n",
    "\n",
    "# Impute with median\n",
    "for c in cols_zero_missing:\n",
    "    if c in df.columns:\n",
    "        df[c].fillna(df[c].median(), inplace=True)\n",
    "\n",
    "print('Imputation done. Any nulls left?\\n', df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e4183",
   "metadata": {},
   "source": [
    "## Correlation matrix (interactive heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f89114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix interactive heatmap using Plotly\n",
    "corr = df.corr()\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=corr.values,\n",
    "    x=corr.columns,\n",
    "    y=corr.columns,\n",
    "    colorbar=dict(title='corr')\n",
    "))\n",
    "fig.update_layout(title='Feature Correlation Heatmap', width=800, height=700)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c133be",
   "metadata": {},
   "source": [
    "## Feature distributions (interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a621d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive histograms for numerical columns (one by one)\n",
    "num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "for col in num_cols:\n",
    "    fig = px.histogram(df, x=col, nbins=40, title=f'Distribution of {col}', marginal='box')\n",
    "    fig.update_layout(width=800, height=450)\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa65c4f2",
   "metadata": {},
   "source": [
    "## Pairwise scatterplots: top features vs Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots of a few useful features against Outcome\n",
    "target = 'Outcome' if 'Outcome' in df.columns else df.columns[-1]\n",
    "plot_cols = ['Glucose','BMI','Age','Insulin']  # common useful features\n",
    "plot_cols = [c for c in plot_cols if c in df.columns]\n",
    "for c in plot_cols:\n",
    "    fig = px.scatter(df, x=c, y=target, title=f'{c} vs {target}', marginal_y='violin', width=800, height=450)\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5640a65c",
   "metadata": {},
   "source": [
    "## Modeling: prepare dataset (features, target), split and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b6924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X, y\n",
    "target_col = 'Outcome' if 'Outcome' in df.columns else df.columns[-1]\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('X_train shape:', X_train.shape, 'X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e22d6af",
   "metadata": {},
   "source": [
    "## Train models: Logistic Regression & Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a718d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "y_proba_lr = lr.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_proba_rf = rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print('Models trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9976ee46",
   "metadata": {},
   "source": [
    "## Model evaluation & comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(name, y_test, y_pred, y_proba):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc = roc_auc_score(y_test, y_proba)\n",
    "    print(f'-- {name} --')\n",
    "    print('Accuracy:', round(acc,4))\n",
    "    print('Precision:', round(prec,4))\n",
    "    print('Recall:', round(rec,4))\n",
    "    print('F1-score:', round(f1,4))\n",
    "    print('ROC AUC:', round(roc,4))\n",
    "    print('\\nClassification report:\\n', classification_report(y_test, y_pred))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return dict(acc=acc,prec=prec,rec=rec,f1=f1,roc=roc,cm=cm)\n",
    "\n",
    "res_lr = evaluate_model('Logistic Regression', y_test, y_pred_lr, y_proba_lr)\n",
    "res_rf = evaluate_model('Random Forest', y_test, y_pred_rf, y_proba_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd446bb9",
   "metadata": {},
   "source": [
    "## Confusion matrix (interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4092b0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9d6ad34",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose best model by F1-score\n",
    "best_name = 'Random Forest' if res_rf['f1'] >= res_lr['f1'] else 'Logistic Regression'\n",
    "best_pred = y_pred_rf if best_name=='Random Forest' else y_pred_lr\n",
    "best_proba = y_proba_rf if best_name=='Random Forest' else y_proba_lr\n",
    "best_cm = res_rf['cm'] if best_name=='Random Forest' else res_lr['cm']\n",
    "\n",
    "labels = ['Negative','Positive']\n",
    "z = best_cm.tolist()\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(z=z, x=labels, y=labels, hoverongaps=False, showscale=False, text=z, texttemplate=\"%{text}\"))\n",
    "fig.update_layout(title=f'Confusion Matrix - {best_name}', width=600, height=500)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a58129",
   "metadata": {},
   "source": [
    "## ROC Curve (interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90249c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves for both models\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_proba_lr)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_proba_rf)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=fpr_lr, y=tpr_lr, mode='lines', name=f'Logistic Regression (AUC={res_lr[\"roc\"]:.3f})'))\n",
    "fig.add_trace(go.Scatter(x=fpr_rf, y=tpr_rf, mode='lines', name=f'Random Forest (AUC={res_rf[\"roc\"]:.3f})'))\n",
    "fig.add_trace(go.Scatter(x=[0,1], y=[0,1], mode='lines', name='Random', line=dict(dash='dash')))\n",
    "fig.update_layout(title='ROC Curves', xaxis_title='False Positive Rate', yaxis_title='True Positive Rate', width=800, height=600)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a38d0",
   "metadata": {},
   "source": [
    "## Feature importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc5cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest (if available)\n",
    "if hasattr(rf, 'feature_importances_'):\n",
    "    fi = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "    fig = px.bar(fi.reset_index().rename(columns={'index':'feature',0:'importance'}),\n",
    "                 x='importance', y='feature', orientation='h', title='Feature Importance (Random Forest)')\n",
    "    fig.update_layout(width=800, height=500)\n",
    "    fig\n",
    "else:\n",
    "    print('RandomForest feature importances not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4574afd2",
   "metadata": {},
   "source": [
    "## Save model (optional) & Next steps\n",
    "\n",
    "- You can export the trained scaler and model using `joblib` or `pickle` for later inference.\n",
    "- Next improvements: cross-validation, hyperparameter tuning (GridSearchCV), SHAP explanations, and deployment via FastAPI.\n",
    "\n",
    "----\n",
    "\n",
    "**End of notebook.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
